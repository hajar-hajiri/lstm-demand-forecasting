# ğŸ“¦ Retail Demand Forecasting â€” LSTM vs LightGBM (End-to-End)



PrÃ©vision de la demande (ventes quotidiennes) sur un jeu de donnÃ©es retail multi-sÃ©ries (\*\*store Ã— item\*\*).  

Le dÃ©pÃ´t contient un pipeline complet : prÃ©paration des donnÃ©es, crÃ©ation de fenÃªtres temporelles, entraÃ®nement dâ€™un modÃ¨le sÃ©quentiel (LSTM), baselines tabulaires, Ã©valuation et gÃ©nÃ©ration de figures.



---



## ğŸ—‚ï¸ Dataset

Kaggle â€” \*Store Item Demand Forecasting Challenge\*  

Colonnes : `date`, `store`, `item`, `sales`



Place `train.csv` ici :



```text

data/train.csv
````


## ğŸ§± Project Structure



```text

lstm-demand-forecasting/

â”œâ”€â”€ configs/

â”‚   â””â”€â”€ default.yaml

â”œâ”€â”€ src/

â”‚   â”œâ”€â”€ data/

â”‚   â”‚   â”œâ”€â”€ make\_dataset.py

â”‚   â”‚   â””â”€â”€ windowing.py

â”‚   â”œâ”€â”€ models/

â”‚   â”‚   â”œâ”€â”€ baselines.py

â”‚   â”‚   â”œâ”€â”€ lgbm\_baseline.py

â”‚   â”‚   â””â”€â”€ lstm\_model.py

â”‚   â”œâ”€â”€ train.py

â”‚   â”œâ”€â”€ evaluate.py

â”‚   â”œâ”€â”€ predict.py

â”‚   â””â”€â”€ utils.py

â”œâ”€â”€ tests/

â”‚   â””â”€â”€ test\_windowing.py

â”œâ”€â”€ artifacts/               # generated after training

â”œâ”€â”€ reports/

â”‚   â””â”€â”€ figures/             # generated plots

â”œâ”€â”€ requirements.txt

â””â”€â”€ README.md
````


---



## âš™ï¸ Installation



```bash

python -m venv .venv

# Windows

.venv\\Scripts\\activate

# Mac/Linux

source .venv/bin/activate



pip install -r requirements.txt

````

---



## ğŸš€ Utilisation



### 1) EntraÃ®ner le modÃ¨le LSTM

```bash

python -m src.train --config configs/default.yaml

````



Fichiers gÃ©nÃ©rÃ©s :



\* `artifacts/model.keras`

\* `artifacts/meta.json` (mÃ©tadonnÃ©es + scaler)



### 2) Ã‰valuer et comparer les modÃ¨les



```bash

python -m src.evaluate --config configs/default.yaml

```



Fichier gÃ©nÃ©rÃ© :



* `reports/figures/forecast\_comparison.png`



### 3) Produire une prÃ©diction (ex: J+90)



```bash

python -m src.predict --config configs/default.yaml --store 1 --item 1 --start-date 2017-10-01 --horizon-days 90

```



Fichier gÃ©nÃ©rÃ© :



\* `artifacts/forecast.csv`



---



## ğŸ§  Approche



### FenÃªtrage (supervisÃ©)



Pour chaque sÃ©rie `(store, item)` :



\* \*\*EntrÃ©e\*\* : 28 jours dâ€™historique (`lookback=28`)

\* \*\*Sortie\*\* : 7 jours Ã  prÃ©dire (`horizon=7`)



Features calendaires ajoutÃ©es : `day-of-week`, `month`, `is\_weekend`, `day`.



---



## ğŸ§© ModÃ¨les



### Baselines



\* \*\*Naive(last)\*\* : rÃ©pÃ¨te la derniÃ¨re valeur observÃ©e

\* \*\*MA(7)\*\* : moyenne mobile sur 7 jours

\* \*\*LightGBM (lags)\*\* : features de lâ€™historique (`last`, `mean7`, `mean14`, `std7`, `trend`) + apprentissage direct multi-horizon



### LSTM (TensorFlow/Keras)



ModÃ¨le sÃ©quentiel global (un seul modÃ¨le pour toutes les sÃ©ries) :



\* `sales\_seq` (standardisÃ©e)

\* `cal\_seq` (features calendaires)

\* embeddings `store\_id` / `item\_id` pour capturer les effets spÃ©cifiques Ã  chaque sÃ©rie



Sortie : vecteur de taille 7 (forecast multi-step).



---



## ğŸ“Š RÃ©sultats (test split)



| ModÃ¨le                     |       MAE |      RMSE |     sMAPE |

| -------------------------- | --------: | --------: | --------: |

| Naive(last)                |    54.677 |    61.302 |     1.975 |

| MA(7)                      |    54.661 |    61.288 |     1.976 |

| LightGBM (lags)            |     8.415 |    11.221 |     0.169 |

| LSTM (global + embeddings) | \*\*6.214\*\* | \*\*8.185\*\* | \*\*0.129\*\* |



---



## ğŸ–¼ï¸ Visualisations



Les figures sont gÃ©nÃ©rÃ©es automatiquement lors de lâ€™Ã©valuation et sauvegardÃ©es dans `reports/figures/`.



### 1) Comparaison des modÃ¨les (horizon 7 jours)

\*\*Figure :\*\* `reports/figures/forecast\_comparison.png`  

Comparaison sur un exemple alÃ©atoire :

\- \*\*True\*\* : ventes rÃ©elles sur les 7 jours Ã  prÃ©dire  

\- \*\*LSTM\*\* : prÃ©vision multi-horizon (7 jours)  

\- \*\*Baselines\*\* : Naive(last), MA(7), LightGBM (lags)



> Objectif : visualiser rapidement lâ€™Ã©cart entre les modÃ¨les, et repÃ©rer les cas oÃ¹ le LSTM capte (ou non) la dynamique.



!\[Forecast comparison](reports/figures/forecast\_comparison.png)



### 2) MÃ©triques dâ€™Ã©valuation (console)

Lors de `python -m src.evaluate ...`, le script affiche :

\- \*\*MAE\*\* (Mean Absolute Error)

\- \*\*RMSE\*\* (Root Mean Squared Error)

\- \*\*sMAPE\*\* (Symmetric Mean Absolute Percentage Error)



Ces mÃ©triques permettent de comparer les approches sur le split test, avec une lecture â€œbusinessâ€ (erreur absolue moyenne) et une lecture â€œstabilitÃ©â€ (RMSE).



---



## ğŸ§ª Tests



```bash

pytest -q

```



---



## ğŸ”§ Configuration



Les principaux paramÃ¨tres sont dans `configs/default.yaml` :



\* `lookback`, `horizon`

\* split temporel (`train\_end`, `val\_end`, `test\_end`)

\* hyperparamÃ¨tres (`epochs`, `batch\_size`, `lstm\_units`, `dropout`...)



---



## ğŸ”­ Next steps (roadmap)



\- Ajouter des modÃ¨les SOTA de forecasting (ex: TFT / PatchTST / N-BEATS) et comparer aux baselines.

\- PrÃ©diction probabiliste (intervalles P50/P90) pour quantifier lâ€™incertitude.

\- Monitoring : drift sur la distribution des ventes et recalibrage pÃ©riodique.

\- Feature store lÃ©ger : lags + Ã©vÃ©nements (promotions, jours fÃ©riÃ©s) quand disponibles.





